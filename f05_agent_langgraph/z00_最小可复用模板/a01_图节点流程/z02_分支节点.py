'''å¤šagent åˆ†æ”¯åˆ¤æ–­å¯¹è¯ '''

# å°†å½“å‰ç›®å½•åŠ å…¥æœç´¢è·¯å¾„
import os
import sys
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# å¯¼å…¥ç¯å¢ƒå˜é‡
from dotenv import load_dotenv
load_dotenv()

# å¯¼å…¥åŒ…
from typing_extensions import TypedDict
from typing import Literal

# å›¾èŠ‚ç‚¹ç®¡ç†
from langgraph.graph import StateGraph, START, END

# èŠå¤©
from langchain_openai import ChatOpenAI
from langgraph.graph import MessagesState

# è®°å¿†
from langgraph.checkpoint.memory import MemorySaver

# å·¥å…·
from langchain_core.tools import tool
from langgraph.prebuilt import ToolNode

# åæ€
from langchain_community.tools.tavily_search import TavilySearchResults

# å¯¼å…¥ç¯å¢ƒå˜é‡
api_url = os.getenv("LLM_URL")
api_key = os.getenv("LLM_API_KEY")
model = os.getenv("LLM_MODEL")

# â€”â€”â€”â€”â€”â€”â€”â€”â€”å…¬å…±å˜é‡â€”â€”â€”â€”â€”â€”â€”â€”â€”
llm = ChatOpenAI(model=model, api_key=api_key, base_url=api_url)  

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”å®šä¹‰å‡½æ•°â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# ===èŠå¤© å‡½æ•°
def researcher(state: MessagesState):
    """ç ”ç©¶å‘˜ï¼šè´Ÿè´£ä¿¡æ¯æ”¶é›†"""
    system_prompt = '''ä½ æ˜¯èµ„æ·±ç ”ç©¶å‘˜ï¼Œæ“…é•¿æ”¶é›†å’Œåˆ†æè¡Œä¸šä¿¡æ¯ã€‚è¯·æä¾›æ•°æ®å’Œè¶‹åŠ¿åˆ†æã€‚ '''
    messages = [{"role": "system", "content": system_prompt}] + state["messages"]
    print('ğŸ” ç ”ç©¶å‘˜ï¼šæ­£åœ¨æ”¶é›†ä¿¡æ¯...')
    return {"messages": [llm.invoke(messages)]}

def chart_analyst(state: MessagesState):
    """å›¾è¡¨åˆ†æå¸ˆï¼šè´Ÿè´£æ•°æ®å¯è§†åŒ–å»ºè®®"""
    system_prompt = '''ä½ æ˜¯æ•°æ®å¯è§†åŒ–ä¸“å®¶ï¼Œæ“…é•¿å°†æ•°æ®è½¬åŒ–ä¸ºå›¾è¡¨å»ºè®®ã€‚è¯·æ¨èåˆé€‚çš„å›¾è¡¨ç±»å‹å’Œå…³é”®æŒ‡æ ‡ã€‚ '''
    messages = [{"role": "system", "content": system_prompt}] + state["messages"]
    print('ğŸ” å›¾è¡¨åˆ†æå¸ˆï¼šæ­£åœ¨åˆ†ææ•°æ®...')
    return {"messages": [llm.invoke(messages)]}

def report_writer(state: MessagesState):
    """æŠ¥å‘Šæ’°å†™å‘˜ï¼šæ•´åˆä¿¡æ¯å¹¶ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
    system_prompt = '''ä½ æ˜¯ä¸“ä¸šæŠ¥å‘Šæ’°å†™å‘˜ï¼Œæ“…é•¿å°†ç ”ç©¶ç»“æœå’Œå›¾è¡¨å»ºè®®æ•´åˆæˆç»“æ„æ¸…æ™°çš„æŠ¥å‘Šã€‚ '''
    messages = [{"role": "system", "content": system_prompt}] + state["messages"]
    print('ğŸ” æŠ¥å‘Šæ’°å†™å‘˜ï¼šæ­£åœ¨æ’°å†™æŠ¥å‘Š...')
    return {"messages": [llm.invoke(messages)]}


# åˆ¤æ–­æ–¹å‘å‡½æ•°
# Literal ç±»å‹æç¤º é™åˆ¶è¿”å›å€¼åªèƒ½è¿”å›ç‰¹å®šçš„æ•°å€¼
def supervisor(state: MessagesState) -> Literal["researcher", "chart_analyst", "report_writer", "end"]:
    """ç®¡ç†è€…ï¼šåè°ƒå„ä¸ª Agent çš„å·¥ä½œæµç¨‹"""
    messages = state["messages"]

    # ç®€å•çš„çŠ¶æ€æœºé€»è¾‘
    # æ ¹æ®å½“å‰å¯¹è¯ä¸­aiå·²ç»ç›¸åº”çš„æ¬¡æ•° å†³å®šä¸‹ä¸€æ­¥æ‰§è¡Œå“ªä¸ªagent
    response_count = len([m for m in messages if m.type == "ai"])  # åªè®¡ç®— AI æ¶ˆæ¯

    if response_count == 0:
        return "researcher"  # ç¬¬ä¸€æ­¥ï¼šç ”ç©¶
    elif response_count == 1:
        return "chart_analyst"  # ç¬¬äºŒæ­¥ï¼šå›¾è¡¨åˆ†æ
    elif response_count == 2:
        return "report_writer"  # ç¬¬ä¸‰æ­¥ï¼šæŠ¥å‘Šæ’°å†™
    else:
        return "end"  # å®Œæˆ



# â€”â€”â€”â€”â€”â€”â€”â€”â€”å®šä¹‰è¿è¡Œæµç¨‹â€”â€”â€”â€”â€”â€”â€”â€”â€”
# åˆ›å»ºå›¾
graph = StateGraph(MessagesState)

# æ³¨å†ŒèŠ‚ç‚¹
graph.add_node("researcher", researcher)
graph.add_node("chart_analyst", chart_analyst)
graph.add_node("report_writer", report_writer)

# å…¥å£è¾¹
graph.add_conditional_edges(
    START, 
    supervisor,
    
    {
        "researcher": "researcher", 
        "chart_analyst": "chart_analyst", 
        "report_writer": "report_writer", 
        "end": END
    }
)

# å‡ºå£è¾¹
# æ¡ä»¶è¾¹ - æ‰€æœ‰èŠ‚ç‚¹éƒ½ç”± supervisor å†³å®šä¸‹ä¸€æ­¥
graph.add_conditional_edges(
    "researcher", 
    supervisor,
    {"researcher": "researcher", 
    "chart_analyst": "chart_analyst", 
    "report_writer": "report_writer", 
    "end": END
    }
)
graph.add_conditional_edges(
    "chart_analyst", 
    supervisor,
    {"researcher": "researcher", 
    "chart_analyst": "chart_analyst", 
    "report_writer": "report_writer", 
    "end": END
    }
)
graph.add_conditional_edges(
    "report_writer", 
    supervisor,
    {"researcher": "researcher", 
    "chart_analyst": "chart_analyst", 
    "report_writer": "report_writer", 
    "end": END
    }
)


# â€”â€”â€”â€”â€”â€”â€”â€”â€”è¿è¡Œâ€”â€”â€”â€”â€”â€”â€”â€”â€”
# ç¼–è¯‘å›¾
app = graph.compile()

# ä¸€æ¬¡æ€§æ‰§è¡Œ
user_input = 'è¯·å¸®æˆ‘åˆ†æä¸€ä¸‹ 2024 å¹´ç”Ÿæˆå¼ AI å¸‚åœºçš„å‘å±•è¶‹åŠ¿ï¼Œå¹¶ç»™å‡ºæŠ¥å‘Š'
res = app.invoke({"messages": [("user", user_input)]})

# æ‰€æœ‰å†å²å¯¹è¯
print("=" * 50 + " å®Œæ•´å¯¹è¯å†å² " + "=" * 50)
for i, msg in enumerate(res["messages"],1):
    print(i,msg.type)
    print(msg)