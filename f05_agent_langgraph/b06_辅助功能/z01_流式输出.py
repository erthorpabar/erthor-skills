# å°†å½“å‰ç›®å½•åŠ å…¥æœç´¢è·¯å¾„
import os
import sys
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# å¯¼å…¥ç¯å¢ƒå˜é‡
from dotenv import load_dotenv
load_dotenv()

api_url = os.getenv("LLM_URL")
api_key = os.getenv("LLM_API_KEY")
model = os.getenv("LLM_MODEL")

import asyncio

# å›¾èŠ‚ç‚¹ç®¡ç†
from langgraph.graph import StateGraph, START, END

# èŠå¤©
from langchain_openai import ChatOpenAI
from langgraph.graph import MessagesState

# è®°å¿†
from langgraph.checkpoint.memory import MemorySaver

# â€”â€”â€”â€”â€”â€”â€”â€”â€”å…¬å…±å˜é‡â€”â€”â€”â€”â€”â€”â€”â€”â€”
llm = ChatOpenAI(model=model, api_key=api_key, base_url=api_url)  

# â€”â€”â€”â€”â€”â€”â€”â€”â€”å®šä¹‰å‡½æ•°â€”â€”â€”â€”â€”â€”â€”â€”â€”
def chat(state: MessagesState):
    return {"messages": [llm.invoke(state["messages"])]}

# â€”â€”â€”â€”â€”â€”â€”â€”â€”å®šä¹‰è¿è¡Œæµç¨‹â€”â€”â€”â€”â€”â€”â€”â€”â€”
# åˆ›å»ºå›¾
graph = StateGraph(MessagesState)

# æ³¨å†ŒèŠ‚ç‚¹
graph.add_node("chat", chat)

# å…¥å£è¾¹
graph.add_edge(START, "chat")

# å‡ºå£è¾¹
graph.add_edge("chat", END)

# â€”â€”â€”â€”â€”â€”â€”â€”â€”è¿è¡Œâ€”â€”â€”â€”â€”â€”â€”â€”â€”
# è®°å¿†
memory = MemorySaver()

# ç¼–è¯‘å›¾
app = graph.compile()

# é…ç½®
config = {"configurable": {"thread_id": "user_001"}}

# â€”â€”â€”â€”â€”â€”â€”â€”â€”å¤šè½®å¯¹è¯æµå¼è¾“å‡ºâ€”â€”â€”â€”â€”â€”â€”â€”â€”
async def chat_with_streaming():
    print("ğŸ‘‹ æ¬¢è¿ä½¿ç”¨AIå¯¹è¯åŠ©æ‰‹ï¼ˆæµå¼è¾“å‡ºï¼‰è¾“å…¥ 'quit' é€€å‡ºå¯¹è¯ã€‚\n")
    
    while True:
        user_input = input("ç”¨æˆ·: ")
        
        # åˆ¤æ–­æ˜¯å¦é€€å‡º
        if user_input.lower() == "quit":
            print("ğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨AIå¯¹è¯åŠ©æ‰‹!")
            break
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºç©º
        if not user_input.strip():
            continue
        
        # æµå¼è¾“å‡º
        print("åŠ©æ‰‹: ", end="", flush=True)
        
        async for event in app.astream_events(
            {"messages": [{"role": "user", "content": user_input}]},
            config=config,
            version="v2"
        ):
            kind = event.get("event")
            
            if kind == "on_chat_model_stream":
                content = event["data"]["chunk"].content
                if content:
                    print(content, end="", flush=True)
        
        print("\n")  # æ¢è¡Œ

# è¿è¡Œ
if __name__ == "__main__":
    asyncio.run(chat_with_streaming())