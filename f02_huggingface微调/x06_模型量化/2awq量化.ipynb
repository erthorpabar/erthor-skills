{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awq import AutoAWQForCausalLM \n",
    "#pip install AutoAWQ\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\env_config\\py\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7a579222c7b48838aab54f895b27764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''对不重要的层进行量化'''\n",
    "\n",
    "model_id = \"facebook/opt-125m\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoAWQForCausalLM.from_pretrained(\n",
    "    # 模型路径\n",
    "    model_id, \n",
    "\n",
    "    # 参数与显存\n",
    "    device_map={\"\": 0}, # k代表参数名前缀，空代表所有模型参数 v代表在哪张gpu上运行\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "AWQ: 100%|██████████| 12/12 [00:32<00:00,  2.73s/it]\n"
     ]
    }
   ],
   "source": [
    "# 量化模型\n",
    "model.quantize(\n",
    "    tokenizer, \n",
    "    quant_config = {\n",
    "        \"zero_point\": True,\n",
    "        \"q_group_size\": 128,\n",
    "        \"w_bit\": 4,\n",
    "        \"version\": \"GEMM\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('opt-125m-awq\\\\tokenizer_config.json',\n",
       " 'opt-125m-awq\\\\special_tokens_map.json',\n",
       " 'opt-125m-awq\\\\vocab.json',\n",
       " 'opt-125m-awq\\\\merges.txt',\n",
       " 'opt-125m-awq\\\\added_tokens.json',\n",
       " 'opt-125m-awq\\\\tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_quantized(\"opt-125m-awq\")\n",
    "tokenizer.save_pretrained(\"opt-125m-awq\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
