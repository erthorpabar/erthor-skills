{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "0 导入包"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "1 量化加载模型"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\0env_config\\1py\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "94d63b52c13e46718c55474e05fa8b7f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = \"google/gemma-2b-it\" # 模型名字或路径\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "\n",
    "    # 参数与显存\n",
    "    device_map={\"\": 0}, # k代表参数名前缀，空代表所有模型参数 v代表在哪张gpu上运行\n",
    "\n",
    "    # 量化加载配置\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True, # 加载到显存中的精度\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16 # 运算的精度\n",
    "    ),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2 加载lora"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# 这行代码会向原始模型中添加lora\n",
    "ft_model = PeftModel.from_pretrained(model, '4lora\\checkpoint-300',torch_dtype = torch.float16, is_trainable = False,)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3 测试"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# 输入\n",
    "prompt = '''<bos><start_of_turn>user\n",
    "价格区间：11元，商品名称：农夫山泉东方树叶 <end_of_turn>'''\n",
    "\n",
    "token_torch = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\") # str -> token"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"user\\n价格区间：11元，商品名称：农夫山泉东方树叶 '\\nmodel\\n总价：11.5元，1个农夫山泉东方树叶 \"]\n"
     ]
    }
   ],
   "source": [
    "# ft_model 回答\n",
    "# response =ft_model.generate(**token_torch,\n",
    "#                          max_new_tokens=2048, # 生成最大长度\n",
    "#                          do_sample=True, # 是否采样\n",
    "#                          num_return_sequences=1, # ⽣成的序列数量\n",
    "#                          temperature=0.1, # 温度\n",
    "#                          num_beams=1, # 搜索树数量\n",
    "#                          top_p=0.95,).to('cpu')\n",
    "# a = tokenizer.batch_decode(response,skip_special_tokens=True) # token -> str\n",
    "# print(a)\n",
    "\n",
    "# 原始模型回答\n",
    "response =model.generate(**token_torch,\n",
    "                         max_new_tokens=2048, # 生成最大长度\n",
    "                         do_sample=True, # 是否采样\n",
    "                         num_return_sequences=1, # ⽣成的序列数量\n",
    "                         temperature=0.1, # 温度\n",
    "                         num_beams=1, # 搜索树数量\n",
    "                         top_p=0.95,).to('cpu')\n",
    "a = tokenizer.batch_decode(response,skip_special_tokens=True) # token -> str\n",
    "print(a)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "4 保存合并之后的模型"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ft_model.save_pretrained('')\n",
    "# tokenizer.save_pretrained('')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
